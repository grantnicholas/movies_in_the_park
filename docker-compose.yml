version: '3'
services:
  postgres:
    # Not mounting a data volume here, all data will get wiped away on restart of the container
    # Not suitable for production, but useful for testing
    image: "postgres:10.5"
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "until PGPASSWORD=root psql -h 'postgres' -U 'root' -c '\\q'; do echo 'postgres sleeping' && sleep 1; done && echo 'postgres ready' "]
      interval: 30s
      timeout: 10s
      retries: 5  
  airflow_init:
    build: ./airflow/.
    entrypoint: "sh -c"
    # Would normally do another healthcheck here for postgres, but for a dummy app lets just sleep
    command: ["sleep 10 && usr/local/bin/airflow initdb"]
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://root:root@postgres:5432/airflow
    depends_on:
      - "postgres"
    restart: on-failure             
  scheduler:
    build: ./airflow/.
    entrypoint: "sh -c"
    # Would not install a python package at deploytime like this in production, but it sure does make testing easier since you don't have to rebuild a docker image
    command: ["pip install -e /root/airflow/dags/. && /bootstrap.sh scheduler"]
    environment:
      # Secrets should be encrypted and not stored in the git repo 
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://root:root@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__BASE_LOG_FOLDER=/root/airflow/logs
      - DARKSKY_API_KEY=f61928191cd795f296ab8df22ee7db6d
    volumes:
      - ./dags:/root/airflow/dags 
      - ./logs:/root/airflow/logs
      # Using a local data volume here, but could mount an s3 bucket as a volume
      - ./data:/data    
    depends_on:
      - "postgres"  
      - "airflow_init"
    restart: on-failure  
  webserver:
    build: ./airflow/.
    entrypoint: "sh -c"
    # Would not install a python package at deploytime like this in production, but it sure does make testing easier since you don't have to rebuild a docker image
    command: ["pip install -e /root/airflow/dags/. && /bootstrap.sh webserver"] 
    environment:
      # Secrets should be encrypted and not stored in the git repo 
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://root:root@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__BASE_LOG_FOLDER=/root/airflow/logs
      - DARKSKY_API_KEY=f61928191cd795f296ab8df22ee7db6d
    volumes:
      - ./dags:/root/airflow/dags
      - ./logs:/root/airflow/logs  
      # Using a local data volume here, but could mount an s3 bucket as a volume
      - ./data:/data     
    ports:
      - "8080:8080"
    depends_on:
      - "postgres"  
      - "airflow_init"
    restart: on-failure  